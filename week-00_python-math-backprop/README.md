# ðŸ§  Week 0 â€“ Python + Math Boost + Backprop from Scratch

Welcome to Week 0 of my 12-week Machine Learning Journey!  
This week is all about building a **strong foundation** in Python, core ML math, and understanding how neural networks learn â€” by coding backpropagation from scratch.

---

## ðŸ“Œ Objectives
- Refresh Python fundamentals (NumPy, OOP, functions)
- Understand **gradient descent**, **chain rule**, and **derivatives**
- Implement a **2-layer neural network** using NumPy (no ML libraries)

---

## ðŸ§  Concepts Covered
- Gradient Descent
- Loss functions (MSE)
- Matrix operations & shapes
- Backpropagation (Chain rule for weight updates)
- Python tricks for ML: broadcasting, indexing, plotting

---

## ðŸ› ï¸ Projects

### 1. `gradient_descent_numpy.py`
A basic implementation of linear regression using manual gradient descent.

### 2. `backprop_from_scratch.py`
A 2-layer neural network trained on synthetic data using only NumPy.

---

## ðŸ“Š Visuals
> _Coming soon: plots showing loss decreasing over time._

- `loss_plot.png` â€“ Line graph of training loss vs. epochs

---

## ðŸ§° Tools & Libraries
- Python 3.10+
- NumPy
- Matplotlib (for loss plots)

Install with:
```bash
pip install -r requirements.txt
