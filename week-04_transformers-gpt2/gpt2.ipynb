{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6ccb20",
   "metadata": {},
   "source": [
    "**Importing libraries and configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf7253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os, math, random, numpy as  np, torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline)\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "model_id   = \"gpt2\"          # or \"distilgpt2\" for faster runs\n",
    "out_dir    = \"runs/jokes_gpt2\"  #output directory of runs\n",
    "block_size = 256             # reduce if you hit OOM (e.g., 128/192)\n",
    "train_bs   = 16              # reduce if OOM (8)\n",
    "eval_bs    = 16\n",
    "epochs     = 3\n",
    "lr         = 5e-5\n",
    "use_fp16   = torch.cuda.is_available()  # safe on your 4070\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23039e6a",
   "metadata": {},
   "source": [
    "\n",
    "**Load dataset of Reddit jokes & create a single text fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75642ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 812593\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 90289\n",
      "    })\n",
      "})\n",
      "{'text': \"What do you call a white supremacist that doesn't eat meat?\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the dataset (big Reddit jokes dump)\n",
    "ds = load_dataset(\"SocialGrep/one-million-reddit-jokes\", split=\"train\")\n",
    "\n",
    "# Merge title + body into one \"text\" field\n",
    "def merge(rec):\n",
    "    title = rec.get(\"title\") or \"\"\n",
    "    body = rec.get(\"body\") or \"\"\n",
    "    txt = (title + \"\\n\" + body).strip()\n",
    "    return {\"text\": txt}\n",
    "\n",
    "# Apply the merge and clean up\n",
    "ds = ds.map(merge, remove_columns=ds.column_names)\n",
    "ds = ds.filter(lambda x: isinstance(x[\"text\"], str) and len(x[\"text\"]) > 20)\n",
    "\n",
    "# 90/10 train/val split\n",
    "split = ds.train_test_split(test_size=0.1, seed=42)\n",
    "ds = DatasetDict({\"train\": split[\"train\"], \"validation\": split[\"test\"]})\n",
    "\n",
    "print(ds)\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d90279",
   "metadata": {},
   "source": [
    "**Tokenizer BPE & Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a1437b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 812593\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 90289\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token  # GPT-2 has no pad token by default\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tok(batch[\"text\"], truncation=True, max_length=block_size)\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0ae5c",
   "metadata": {},
   "source": [
    "**Load model + collator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "981df9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.resize_token_embeddings(len(tok))  # in case PAD was added\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83853a",
   "metadata": {},
   "source": [
    "**Training setup + fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f88294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TochiBuilds\\AppData\\Local\\Temp\\ipykernel_3412\\2988788813.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101575' max='101575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101575/101575 9:22:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.763700</td>\n",
       "      <td>3.955393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.649500</td>\n",
       "      <td>3.809327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.513500</td>\n",
       "      <td>3.716680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.465300</td>\n",
       "      <td>3.649855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.462900</td>\n",
       "      <td>3.601258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.440300</td>\n",
       "      <td>3.583766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.374100</td>\n",
       "      <td>3.551235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.373200</td>\n",
       "      <td>3.530006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.378000</td>\n",
       "      <td>3.506742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.316300</td>\n",
       "      <td>3.495456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.276200</td>\n",
       "      <td>3.478090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.261100</td>\n",
       "      <td>3.462467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.322700</td>\n",
       "      <td>3.448474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.278700</td>\n",
       "      <td>3.438176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.227000</td>\n",
       "      <td>3.431896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.200500</td>\n",
       "      <td>3.423141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.264500</td>\n",
       "      <td>3.413447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.173900</td>\n",
       "      <td>3.405268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.191600</td>\n",
       "      <td>3.402535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.158200</td>\n",
       "      <td>3.394851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.152600</td>\n",
       "      <td>3.389887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.223500</td>\n",
       "      <td>3.380404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.199700</td>\n",
       "      <td>3.377554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.186100</td>\n",
       "      <td>3.371124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.155800</td>\n",
       "      <td>3.363959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.239800</td>\n",
       "      <td>3.362622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.141500</td>\n",
       "      <td>3.354230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.148900</td>\n",
       "      <td>3.352789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.141900</td>\n",
       "      <td>3.347701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.222500</td>\n",
       "      <td>3.344754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.118300</td>\n",
       "      <td>3.342806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.103800</td>\n",
       "      <td>3.333081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.153500</td>\n",
       "      <td>3.329587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.206400</td>\n",
       "      <td>3.324557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.138400</td>\n",
       "      <td>3.322084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.095200</td>\n",
       "      <td>3.319798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.169100</td>\n",
       "      <td>3.314170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.109900</td>\n",
       "      <td>3.314245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.098800</td>\n",
       "      <td>3.313311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.147900</td>\n",
       "      <td>3.304634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>3.200800</td>\n",
       "      <td>3.302213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.131300</td>\n",
       "      <td>3.301703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>3.140500</td>\n",
       "      <td>3.295805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.163000</td>\n",
       "      <td>3.290795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>3.092400</td>\n",
       "      <td>3.292138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.092500</td>\n",
       "      <td>3.287831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>3.283972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.072800</td>\n",
       "      <td>3.284644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>3.017500</td>\n",
       "      <td>3.282269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.107300</td>\n",
       "      <td>3.276866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.275098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.083500</td>\n",
       "      <td>3.272410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>3.073300</td>\n",
       "      <td>3.269002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.071400</td>\n",
       "      <td>3.268079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>3.031400</td>\n",
       "      <td>3.266909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.149200</td>\n",
       "      <td>3.263134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>3.094600</td>\n",
       "      <td>3.259176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.112500</td>\n",
       "      <td>3.259866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>3.040800</td>\n",
       "      <td>3.256580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.084900</td>\n",
       "      <td>3.253384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>3.070500</td>\n",
       "      <td>3.249473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.130800</td>\n",
       "      <td>3.250221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>3.066300</td>\n",
       "      <td>3.247952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.083500</td>\n",
       "      <td>3.244423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>3.066900</td>\n",
       "      <td>3.242626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.180500</td>\n",
       "      <td>3.239437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>3.014900</td>\n",
       "      <td>3.241381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.019200</td>\n",
       "      <td>3.237590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>3.034300</td>\n",
       "      <td>3.232272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.063600</td>\n",
       "      <td>3.229893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>3.062600</td>\n",
       "      <td>3.228084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.147200</td>\n",
       "      <td>3.229034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>3.038400</td>\n",
       "      <td>3.227247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.025800</td>\n",
       "      <td>3.224514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>3.040200</td>\n",
       "      <td>3.221501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.089400</td>\n",
       "      <td>3.222439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>3.012300</td>\n",
       "      <td>3.219371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.012200</td>\n",
       "      <td>3.217580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>3.071800</td>\n",
       "      <td>3.215923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.058800</td>\n",
       "      <td>3.214267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>3.032600</td>\n",
       "      <td>3.212743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.986200</td>\n",
       "      <td>3.214722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>3.060700</td>\n",
       "      <td>3.207203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.997200</td>\n",
       "      <td>3.207390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>3.006900</td>\n",
       "      <td>3.203655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>3.046300</td>\n",
       "      <td>3.203181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>3.081600</td>\n",
       "      <td>3.199502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>3.030600</td>\n",
       "      <td>3.200075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>3.196174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>3.065000</td>\n",
       "      <td>3.195643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>2.980900</td>\n",
       "      <td>3.197067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>3.059800</td>\n",
       "      <td>3.193852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>3.146900</td>\n",
       "      <td>3.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.945400</td>\n",
       "      <td>3.193126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>2.989200</td>\n",
       "      <td>3.190823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.951000</td>\n",
       "      <td>3.188864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>3.185891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>3.043800</td>\n",
       "      <td>3.185052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>3.062800</td>\n",
       "      <td>3.181978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.122600</td>\n",
       "      <td>3.179461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>2.919400</td>\n",
       "      <td>3.178504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>2.986700</td>\n",
       "      <td>3.178405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>2.998800</td>\n",
       "      <td>3.176651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>3.005000</td>\n",
       "      <td>3.176181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>2.997700</td>\n",
       "      <td>3.176991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>2.977400</td>\n",
       "      <td>3.173856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>2.980500</td>\n",
       "      <td>3.174287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>3.000400</td>\n",
       "      <td>3.170995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>2.976100</td>\n",
       "      <td>3.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>2.951900</td>\n",
       "      <td>3.167145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>2.954500</td>\n",
       "      <td>3.167289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>3.091600</td>\n",
       "      <td>3.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>3.038200</td>\n",
       "      <td>3.163263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>3.060200</td>\n",
       "      <td>3.161594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>2.916400</td>\n",
       "      <td>3.161628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>2.907900</td>\n",
       "      <td>3.159425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>2.947400</td>\n",
       "      <td>3.157517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>2.993500</td>\n",
       "      <td>3.155749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>2.973200</td>\n",
       "      <td>3.155457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>2.949800</td>\n",
       "      <td>3.153270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>2.925700</td>\n",
       "      <td>3.153651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>2.981200</td>\n",
       "      <td>3.151843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>2.864700</td>\n",
       "      <td>3.151752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>2.984600</td>\n",
       "      <td>3.149214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>2.915100</td>\n",
       "      <td>3.148496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>2.931000</td>\n",
       "      <td>3.147055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>2.980200</td>\n",
       "      <td>3.145620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>2.953700</td>\n",
       "      <td>3.145173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>2.918600</td>\n",
       "      <td>3.144336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>3.026000</td>\n",
       "      <td>3.142585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>3.010700</td>\n",
       "      <td>3.141284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.140588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>2.976800</td>\n",
       "      <td>3.138771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>2.938800</td>\n",
       "      <td>3.138549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>2.937700</td>\n",
       "      <td>3.138109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>2.977100</td>\n",
       "      <td>3.136295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>2.938500</td>\n",
       "      <td>3.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>3.004500</td>\n",
       "      <td>3.134454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>2.965500</td>\n",
       "      <td>3.133955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.031700</td>\n",
       "      <td>3.132448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>2.921700</td>\n",
       "      <td>3.132311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>3.046800</td>\n",
       "      <td>3.130655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>2.952800</td>\n",
       "      <td>3.130994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>3.129768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.128271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>2.933500</td>\n",
       "      <td>3.128110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>2.954500</td>\n",
       "      <td>3.126737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>2.970900</td>\n",
       "      <td>3.125630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>2.987100</td>\n",
       "      <td>3.124677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>2.956700</td>\n",
       "      <td>3.125103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>2.991200</td>\n",
       "      <td>3.122892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>2.955900</td>\n",
       "      <td>3.122904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>2.934800</td>\n",
       "      <td>3.122515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>3.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>2.919500</td>\n",
       "      <td>3.121844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>2.909700</td>\n",
       "      <td>3.120930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>2.962200</td>\n",
       "      <td>3.119930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>2.827900</td>\n",
       "      <td>3.120263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>2.879100</td>\n",
       "      <td>3.118440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.003800</td>\n",
       "      <td>3.117321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>2.961800</td>\n",
       "      <td>3.117324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>2.959800</td>\n",
       "      <td>3.116869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>2.991800</td>\n",
       "      <td>3.115943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>2.864900</td>\n",
       "      <td>3.117575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>2.937000</td>\n",
       "      <td>3.115948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>2.924500</td>\n",
       "      <td>3.115084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>3.036400</td>\n",
       "      <td>3.113922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>2.958400</td>\n",
       "      <td>3.113540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>2.928200</td>\n",
       "      <td>3.113993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>2.976400</td>\n",
       "      <td>3.113182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>2.931300</td>\n",
       "      <td>3.112909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>2.995000</td>\n",
       "      <td>3.112706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>2.930700</td>\n",
       "      <td>3.112060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>3.012100</td>\n",
       "      <td>3.111313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87500</td>\n",
       "      <td>2.919200</td>\n",
       "      <td>3.110886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>2.921100</td>\n",
       "      <td>3.110711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88500</td>\n",
       "      <td>2.943100</td>\n",
       "      <td>3.110830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89000</td>\n",
       "      <td>2.904100</td>\n",
       "      <td>3.110232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89500</td>\n",
       "      <td>2.917200</td>\n",
       "      <td>3.110116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>2.893800</td>\n",
       "      <td>3.110125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90500</td>\n",
       "      <td>2.850800</td>\n",
       "      <td>3.110131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91000</td>\n",
       "      <td>2.894100</td>\n",
       "      <td>3.109674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91500</td>\n",
       "      <td>2.945000</td>\n",
       "      <td>3.109658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>2.913000</td>\n",
       "      <td>3.109532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92500</td>\n",
       "      <td>2.952200</td>\n",
       "      <td>3.109233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>2.952200</td>\n",
       "      <td>3.108905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93500</td>\n",
       "      <td>2.907300</td>\n",
       "      <td>3.108853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94000</td>\n",
       "      <td>2.907500</td>\n",
       "      <td>3.108882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94500</td>\n",
       "      <td>2.882300</td>\n",
       "      <td>3.108684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>2.927900</td>\n",
       "      <td>3.108628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95500</td>\n",
       "      <td>2.921900</td>\n",
       "      <td>3.108531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>2.904600</td>\n",
       "      <td>3.108569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96500</td>\n",
       "      <td>2.858000</td>\n",
       "      <td>3.108477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97000</td>\n",
       "      <td>2.858200</td>\n",
       "      <td>3.108512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97500</td>\n",
       "      <td>2.979500</td>\n",
       "      <td>3.108484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98000</td>\n",
       "      <td>2.971300</td>\n",
       "      <td>3.108420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98500</td>\n",
       "      <td>2.934500</td>\n",
       "      <td>3.108388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>2.898100</td>\n",
       "      <td>3.108353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99500</td>\n",
       "      <td>2.921700</td>\n",
       "      <td>3.108334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>2.957000</td>\n",
       "      <td>3.108328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100500</td>\n",
       "      <td>2.915500</td>\n",
       "      <td>3.108333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101000</td>\n",
       "      <td>2.897100</td>\n",
       "      <td>3.108329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101500</td>\n",
       "      <td>2.960600</td>\n",
       "      <td>3.108325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=101575, training_loss=3.0503543618850277, metrics={'train_runtime': 33762.6425, 'train_samples_per_second': 24.068, 'train_steps_per_second': 3.009, 'total_flos': 9771230898432000.0, 'train_loss': 3.0503543618850277, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=train_bs,\n",
    "    per_device_eval_batch_size=eval_bs,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    fp16=use_fp16,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e05e150",
   "metadata": {},
   "source": [
    "**Evaluate → Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71a3cd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11287' max='11287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11287/11287 02:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.1083250045776367, 'eval_runtime': 137.2822, 'eval_samples_per_second': 657.689, 'eval_steps_per_second': 82.218, 'epoch': 1.0}\n",
      "Perplexity: 22.38\n"
     ]
    }
   ],
   "source": [
    "eval_res = trainer.evaluate()\n",
    "ppl = math.exp(eval_res[\"eval_loss\"])\n",
    "print(eval_res)\n",
    "print(f\"Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98bb7e0",
   "metadata": {},
   "source": [
    "**Save checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c8b425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: runs/jokes_gpt2\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(out_dir)       # saves config + tokenizer + model weights\n",
    "tok.save_pretrained(out_dir)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(out_dir, \"pytorch_model_weights_only.pt\"))\n",
    "print(\"Saved to:\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d196a6",
   "metadata": {},
   "source": [
    "**Generation helper (sampling: temperature / top-k / top-p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae099e10",
   "metadata": {},
   "source": [
    "**Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "043c039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/jokes_gpt2\\\\samples.txt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "samples_path = os.path.join(out_dir, \"samples.txt\")\n",
    "\n",
    "with open(samples_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    prompts = [\n",
    "        \"Why did the chicken cross the road?\",\n",
    "        \"Write a one-line dad joke about GPUs:\",\n",
    "        \"A software engineer and a hardware engineer walk into a bar and\"\n",
    "    ]\n",
    "    for p in prompts:\n",
    "        outs = gen(p, do_sample=True, top_k=50, top_p=0.9, temperature=0.9, max_new_tokens=80, num_return_sequences=3,\n",
    "                   pad_token_id=tok.eos_token_id)\n",
    "        f.write(f\"\\n\\n# Prompt: {p}\\n\")\n",
    "        for i, o in enumerate(outs, 1):\n",
    "            f.write(f\"\\n[{i}] {o['generated_text']}\\n\")\n",
    "samples_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca0e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, re, glob, torch, random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gradio as gr\n",
    "\n",
    "random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# 1) pick weights (best checkpoint if present)\n",
    "out_dir = globals().get(\"out_dir\", \"runs/jokes_gpt2\")\n",
    "model_path = getattr(globals().get(\"trainer\", None), \"state\", None)\n",
    "model_path = getattr(model_path, \"best_model_checkpoint\", None) or \\\n",
    "             (sorted(glob.glob(os.path.join(out_dir, \"checkpoint-*\")), key=os.path.getmtime)[-1]\n",
    "              if glob.glob(os.path.join(out_dir, \"checkpoint-*\")) else out_dir)\n",
    "\n",
    "# 2) load model/tokenizer (GPU if available)\n",
    "tok = globals().get(\"tokenizer\") or globals().get(\"tok\") or AutoTokenizer.from_pretrained(model_path)\n",
    "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "# 3) cleaning + scoring\n",
    "TAG = re.compile(r\"\\[(?:NSFW|NSFL|Long|OC|Serious|Spoiler|Mature|Political)\\]\", re.I)\n",
    "URL = re.compile(r\"https?://\\S+\")\n",
    "def clean(s: str) -> str:\n",
    "    s = TAG.sub(\"\", URL.sub(\"\", s)).replace(\"�\",\"\").strip()\n",
    "    return s.split(\"\\n\", 1)[0].strip()\n",
    "\n",
    "def score(s: str) -> float:\n",
    "    L = len(s)\n",
    "    return -abs(L-90) + (10 if s.endswith(('.', '!', '?', '…')) else 0)\n",
    "\n",
    "# 4) hard block bad words/tokens\n",
    "bad_words = [\"NSFW\",\"NSFL\",\"x-post\",\"crosspost\",\"subreddit\",\"r/\",\"/r/\",\"upvote\",\"downvote\",\"mod\",\"OC\",\"Meta\"]\n",
    "bad_ids = [tok.encode(w, add_special_tokens=False) for w in bad_words]\n",
    "bad_ids = [ids for ids in bad_ids if len(ids) > 0]\n",
    "nl_id = tok.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "\n",
    "def gen_one_topic(topic: str, attempts: int = 10):\n",
    "    system = \"Write a clean, witty ONE-LINER joke. Avoid profanity or adult content.\"\n",
    "    prompt = f\"{system}\\nTopic: {topic}\\nJoke:\"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    cands = []\n",
    "    for _ in range(attempts):\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True, temperature=0.8, top_k=50, top_p=0.9,\n",
    "            repetition_penalty=1.2, no_repeat_ngram_size=4,\n",
    "            max_new_tokens=32,\n",
    "            bad_words_ids=bad_ids,\n",
    "            eos_token_id=[tok.eos_token_id, nl_id],  # stop at newline\n",
    "        )\n",
    "        new_ids = out_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "        text = tok.decode(new_ids, skip_special_tokens=True)\n",
    "        text = clean(text)\n",
    "        if 12 <= len(text) <= 160 and not any(tag in text.lower() for tag in [\"nsfw\",\"nsfl\",\"r/\",\"x-post\"]):\n",
    "            cands.append(text)\n",
    "\n",
    "    cands = sorted(set(cands), key=score, reverse=True)\n",
    "    return cands[:3] or [\"(try another topic)\"]\n",
    "\n",
    "# ---- Simple Gradio app (wider output, no scrolling) ----\n",
    "def ui_fn(topic):\n",
    "    jokes = gen_one_topic(topic, attempts=12)\n",
    "    return \"\\n\".join(f\"- {j}\" for j in jokes)\n",
    "\n",
    "css = \"\"\"\n",
    "#out_md { min-height: 320px; font-size: 1.05rem; }\n",
    ".gradio-container { max-width: 980px !important; }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=css) as demo:\n",
    "    gr.Markdown(\"# Fine-tuned Joke GPT-2\\n\\nClean one-liner jokes. Try a topic!\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            inp = gr.Textbox(label=\"Topic\", placeholder=\"GPUs, exams, roommates...\", lines=1)\n",
    "            submit = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            clear = gr.Button(\"Clear\")\n",
    "        with gr.Column(scale=2):\n",
    "            out = gr.Markdown(label=\"Output\", elem_id=\"out_md\")\n",
    "    submit.click(ui_fn, inputs=inp, outputs=out)\n",
    "    clear.click(lambda: (\"\", \"\"), None, [inp, out])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
